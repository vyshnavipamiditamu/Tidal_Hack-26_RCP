{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e5b6c3-8f56-4fa8-b720-3973bc43e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8655f6-c53d-44d7-ad3b-f5d65205fe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêß Opening ILIDataV2.xlsx...\n",
      "‚úÖ Processed 2007 successfully. Saved to standardized/ILI_2007_cleaned.csv\n",
      "‚úÖ Processed 2015 successfully. Saved to standardized/ILI_2015_cleaned.csv\n",
      "‚úÖ Processed 2022 successfully. Saved to standardized/ILI_2022_cleaned.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Standardization complete.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Since we are working locally in SageMaker, we set this to False\n",
    "IS_AWS = False \n",
    "input_file = \"ILIDataV2.xlsx\"\n",
    "years = [\"2007\", \"2015\", \"2022\"]\n",
    "\n",
    "mappings = {\n",
    "    \"2007\": {\"log dist. [ft]\": \"distance\", \"o'clock\": \"clock\", \"event\": \"feature_type\", \"depth [%]\": \"depth\", \"length [in]\": \"length\", \"width [in]\": \"width\"},\n",
    "    \"2015\": {\"Log Dist. [ft]\": \"distance\", \"O'clock\": \"clock\", \"Event Description\": \"feature_type\", \"Depth [%]\": \"depth\", \"Length [in]\": \"length\", \"Width [in]\": \"width\"},\n",
    "    \"2022\": {\"ILI Wheel Count \\n[ft.]\": \"distance\", \"O'clock\\n[hh:mm]\": \"clock\", \"Event Description\": \"feature_type\", \"Metal Loss Depth \\n[%]\": \"depth\", \"Length [in]\": \"length\", \"Width [in]\": \"width\"}\n",
    "}\n",
    "\n",
    "def run_standardization():\n",
    "    try:\n",
    "        # 1. LOAD DATA Local mode\n",
    "        print(f\"üêß Opening {input_file}...\")\n",
    "        with open(input_file, \"rb\") as f:\n",
    "            excel_data = f.read()\n",
    "\n",
    "        for year in years:\n",
    "            # Read sheet\n",
    "            df = pd.read_excel(io.BytesIO(excel_data), sheet_name=year)\n",
    "            \n",
    "            # Apply Gia's mappings\n",
    "            df = df.rename(columns=mappings[year])\n",
    "            df['survey_year'] = int(year)\n",
    "            \n",
    "            # Keep only the columns needed for alignment\n",
    "            target_cols = [\"distance\", \"clock\", \"feature_type\", \"depth\", \"length\", \"width\", \"survey_year\"]\n",
    "            existing_cols = [c for c in target_cols if c in df.columns]\n",
    "            df_final = df[existing_cols]\n",
    "\n",
    "            # 2. SAVE DATA locally\n",
    "            output_folder = \"standardized\"\n",
    "            output_filename = f\"ILI_{year}_cleaned.csv\"\n",
    "            \n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "            \n",
    "            df_final.to_csv(os.path.join(output_folder, output_filename), index=False)\n",
    "            print(f\"‚úÖ Processed {year} successfully. Saved to {output_folder}/{output_filename}\")\n",
    "\n",
    "        return \"Standardization complete.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return str(e)\n",
    "\n",
    "# Run the process\n",
    "run_standardization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf17d7f-8011-45a2-ac98-a74385aedf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêß Re-aligning with Valve-First Strategy...\n",
      "\n",
      "--- Improved Valve Sanity Check ---\n",
      "2022 Distance: -0.0030 ft\n",
      "2007 Aligned:  -0.0030 ft\n",
      "New Error:     0.0000 ft (Target: < 0.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Load the standardized CSVs\n",
    "df_07 = pd.read_csv('standardized/ILI_2007_cleaned.csv')\n",
    "df_15 = pd.read_csv('standardized/ILI_2015_cleaned.csv')\n",
    "df_22 = pd.read_csv('standardized/ILI_2022_cleaned.csv')\n",
    "\n",
    "def align_with_master_anchor(df_source, df_ref):\n",
    "    # 1. Find the first Valve in both (This is our \"True Zero\")\n",
    "    anchor_src_val = df_source[df_source['feature_type'].str.contains('Valve', na=False)]['distance'].iloc[0]\n",
    "    anchor_ref_val = df_ref[df_ref['feature_type'].str.contains('Valve', na=False)]['distance'].iloc[0]\n",
    "    \n",
    "    # 2. Filter Girth Welds that appear AFTER the valve\n",
    "    anchors_src = df_source[(df_source['feature_type'].str.contains('Girth', na=False)) & \n",
    "                            (df_source['distance'] > anchor_src_val)]\n",
    "    anchors_ref = df_ref[(df_ref['feature_type'].str.contains('Girth', na=False)) & \n",
    "                         (df_ref['distance'] > anchor_ref_val)]\n",
    "    \n",
    "    # 3. Create staple points (adding the Valve at the start)\n",
    "    old_x = [anchor_src_val] + anchors_src['distance'].tolist()\n",
    "    new_x = [anchor_ref_val] + anchors_ref['distance'].tolist()\n",
    "    \n",
    "    # Match indices\n",
    "    n = min(len(old_x), len(new_x))\n",
    "    transform = interp1d(old_x[:n], new_x[:n], fill_value=\"extrapolate\")\n",
    "    \n",
    "    # 4. Transform the distances\n",
    "    df_source['aligned_dist'] = transform(df_source['distance'])\n",
    "    return df_source\n",
    "\n",
    "# Run the improved alignment\n",
    "print(\"üêß Re-aligning with Valve-First Strategy...\")\n",
    "df_07_final = align_with_master_anchor(df_07, df_22)\n",
    "df_15_final = align_with_master_anchor(df_15, df_22)\n",
    "\n",
    "# --- NEW SANITY CHECK ---\n",
    "v_22 = df_22[df_22['feature_type'].str.contains('Valve', na=False)]['distance'].iloc[0]\n",
    "v_07_aligned = df_07_final[df_07_final['feature_type'].str.contains('Valve', na=False)]['aligned_dist'].iloc[0]\n",
    "\n",
    "print(f\"\\n--- Improved Valve Sanity Check ---\")\n",
    "print(f\"2022 Distance: {v_22:.4f} ft\")\n",
    "print(f\"2007 Aligned:  {v_07_aligned:.4f} ft\")\n",
    "print(f\"New Error:     {abs(v_22 - v_07_aligned):.4f} ft (Target: < 0.1)\")\n",
    "\n",
    "# Save the final mapped files\n",
    "df_07_final.to_csv('standardized/ILI_2007_aligned.csv', index=False)\n",
    "df_15_final.to_csv('standardized/ILI_2015_aligned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01679269-e300-4037-af74-ea6dac99bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2015 Sanity Check ---\n",
      "Accuracy: 0.0000 ft error\n"
     ]
    }
   ],
   "source": [
    "v_22 = df_22[df_22['feature_type'].str.contains('Valve', na=False)]['distance'].iloc[0]\n",
    "v_15_aligned = df_15_final[df_15_final['feature_type'].str.contains('Valve', na=False)]['aligned_dist'].iloc[0]\n",
    "\n",
    "print(f\"--- 2015 Sanity Check ---\")\n",
    "print(f\"Accuracy: {abs(v_22 - v_15_aligned):.4f} ft error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa55d029-8a2d-4f6e-bc2c-f9d661f58d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pairing attempt finished. Found 187 matches!\n"
     ]
    }
   ],
   "source": [
    "def get_matching_pairs(df1, df2):\n",
    "    # 1. Ensure we are looking for ANY metal loss\n",
    "    # (Matches 'Internal Metal Loss', 'External', etc.)\n",
    "    df1_ml = df1[df1['feature_type'].str.contains('Metal Loss|Loss', na=False, case=False)]\n",
    "    df2_ml = df2[df2['feature_type'].str.contains('Metal Loss|Loss', na=False, case=False)]\n",
    "    \n",
    "    if len(df1_ml) == 0 or len(df2_ml) == 0:\n",
    "        print(\"‚ùå One of the years has no 'Metal Loss' events labeled. Check feature_type names!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Create the Cost Matrix\n",
    "    # We use a 20-foot tolerance because matching rust is harder than matching welds\n",
    "    dist_matrix = np.abs(df1_ml['aligned_dist'].values[:, np.newaxis] - df2_ml['distance'].values)\n",
    "    \n",
    "    # 3. Hungarian Algorithm\n",
    "    row_ind, col_ind = linear_sum_assignment(dist_matrix)\n",
    "    \n",
    "    matches = []\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        # Increased tolerance to 20 feet to find those 'hidden' matches\n",
    "        if dist_matrix[r, c] < 20.0: \n",
    "            matches.append({\n",
    "                'id_2015': r,\n",
    "                'location_22': df2_ml.iloc[c]['distance'],\n",
    "                'dist_diff': dist_matrix[r, c],\n",
    "                'depth_15': df1_ml.iloc[r]['depth'],\n",
    "                'depth_22': df2_ml.iloc[c]['depth']\n",
    "            })\n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# Execute again\n",
    "matched_df = get_matching_pairs(df_15_final, df_22)\n",
    "print(f\"‚úÖ Pairing attempt finished. Found {len(matched_df)} matches!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d824db42-cbbf-4f5c-8b91-923fe2b1827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Corrosion Analysis Summary ---\n",
      "Average Depth in 2015: 17.73%\n",
      "Average Depth in 2022: 18.67%\n",
      "Average Growth Rate:   0.1352% per year\n",
      "üö® DANGER: 20 spots are growing at critical rates!\n"
     ]
    }
   ],
   "source": [
    "# Calculate Growth Rate\n",
    "# 2022 depth minus 2015 depth, divided by the 7-year gap\n",
    "matched_df['total_growth'] = matched_df['depth_22'] - matched_df['depth_15']\n",
    "matched_df['growth_per_year'] = matched_df['total_growth'] / 7\n",
    "\n",
    "# Identify the \"Danger Spots\"\n",
    "# Spots growing faster than 2% depth per year are high priority\n",
    "danger_spots = matched_df[matched_df['growth_per_year'] > 2.0]\n",
    "\n",
    "print(f\"--- Corrosion Analysis Summary ---\")\n",
    "print(f\"Average Depth in 2015: {matched_df['depth_15'].mean():.2f}%\")\n",
    "print(f\"Average Depth in 2022: {matched_df['depth_22'].mean():.2f}%\")\n",
    "print(f\"Average Growth Rate:   {matched_df['growth_per_year'].mean():.4f}% per year\")\n",
    "print(f\"üö® DANGER: {len(danger_spots)} spots are growing at critical rates!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c079fd-97d9-4e31-baef-e5ebc6eecaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üõ°Ô∏è RECOVERED PHASE 2 REPORT ---\n",
      "Clock Position Match Rate: 13.37%\n",
      "Verified Danger Spots:     4 out of 20\n"
     ]
    }
   ],
   "source": [
    "# Cross-Verification Script\n",
    "def fuzzy_clock_match(c1, c2):\n",
    "    try:\n",
    "        # Helper to convert almost any format to \"Minutes from 12:00\"\n",
    "        def to_minutes(val):\n",
    "            val = str(val).strip()\n",
    "            if ':' in val: # Handle \"10:30\"\n",
    "                h, m = map(int, val.split(':')[:2])\n",
    "                return h * 60 + m\n",
    "            return float(val) * 60 # Handle \"10.5\"\n",
    "        \n",
    "        diff = abs(to_minutes(c1) - to_minutes(c2))\n",
    "        return diff <= 30 # Allow 30 mins of \"wobble\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Re-verify with the Fuzzy Logic\n",
    "verified_results = []\n",
    "for _, match in matched_df.iterrows():\n",
    "    row_15 = df_15_final[df_15_final['depth'] == match['depth_15']].iloc[0]\n",
    "    row_22 = df_22[df_22['distance'] == match['location_22']].iloc[0]\n",
    "    \n",
    "    verified_results.append(fuzzy_clock_match(row_15['clock'], row_22['clock']))\n",
    "\n",
    "matched_df['clock_verified'] = verified_results\n",
    "accuracy = (matched_df['clock_verified'].sum() / len(matched_df)) * 100\n",
    "\n",
    "print(f\"--- üõ°Ô∏è RECOVERED PHASE 2 REPORT ---\")\n",
    "print(f\"Clock Position Match Rate: {accuracy:.2f}%\")\n",
    "print(f\"Verified Danger Spots:     {matched_df[(matched_df['clock_verified']) & (matched_df['growth_per_year'] > 2.0)].shape[0]} out of 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3931527-207a-440f-9485-f3775bb16baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP RISK SITES\n",
      " Distance (ft)  Growth Rate (%/yr)  Orientation Verified?\n",
      "     45013.179            2.571429                   True\n",
      "     44813.491            2.428571                   True\n",
      "     15007.881            2.142857                   True\n",
      "     29241.407            2.142857                   True\n",
      "     44887.146            1.285714                   True\n",
      "     45253.973            1.142857                   True\n",
      "     45917.526            1.142857                   True\n",
      "     43850.144            1.000000                   True\n",
      "     43851.601            1.000000                   True\n",
      "     56734.350            0.571429                   True\n"
     ]
    }
   ],
   "source": [
    "# Create the Priority List\n",
    "# We prioritize the 4 verified spots first, then the other high-growth spots\n",
    "priority_list = matched_df.sort_values(by=['clock_verified', 'growth_per_year'], ascending=False).head(10)\n",
    "\n",
    "# Format for the presentation\n",
    "presentation_table = priority_list[['location_22', 'growth_per_year', 'clock_verified']]\n",
    "presentation_table.columns = ['Distance (ft)', 'Growth Rate (%/yr)', 'Orientation Verified?']\n",
    "\n",
    "print(\"TOP RISK SITES\")\n",
    "print(presentation_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e4daf-3f82-404e-a7b5-e3330d3c0e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
